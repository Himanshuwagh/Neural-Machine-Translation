{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df47eCT8961F"
   },
   "source": [
    "# Training Encoder Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0b45f7c-c38d-466f-a5f4-375f09af651d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8c5a33e-fc53-4cca-aeef-0028b69d2ad2"
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"cleaned_MAR.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40397cd7-d8ec-41f1-837f-a3bdeeae704a"
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6052bba-9f6c-4102-94e1-ed6972eae62d"
   },
   "source": [
    "**Note** this data was cleaned at notebook of EDA_And_Cleaning_Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a608cd69-fd28-4b04-bb24-0cdccbf637c0"
   },
   "source": [
    "# Prepare dataset for encoder decoder model\n",
    "## Encoder:-\n",
    "* Here first we will convert text into numbers\n",
    "* [WordEmbedding](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/):- Then embedding is very important layer for beause it will convert the input word-`numbers into more dimension of vectors which will have semantic information words means beause of this we can know which words are similar or near to each other.`\n",
    "* We will not take output of timestamps of encoder beause it will be like one to one mapping so we will just take selt states of encoder as context vector.\n",
    "\n",
    "## Decoder:-\n",
    "* First most important thing is we have to add special tokens in each target language at start SOS and EOS at end reason of this is `The length of translated sentence might not be same as other language so it is to tell model where is start and end of sentence.`\n",
    "* When building model we will provide initial state of model as context vector recived from Encoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3a4e566-74b2-48ff-8f34-0b5c7d92df37"
   },
   "source": [
    "#### Fisrt add eos and sos tokens\n",
    "* SOS = Start Of String\n",
    "* EOS = End Of String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15104303-ee91-4eff-bc9c-ed47916f4481"
   },
   "outputs": [],
   "source": [
    "df.Marathi = df.Marathi.apply(lambda x: 'sos '+ x +' eos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfcaeda0-13f7-499f-ace7-788838c8a193"
   },
   "source": [
    "### create vocabulary of english and marathi words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4b74e168-0c03-4ab3-8147-e4efe7f35b33"
   },
   "outputs": [],
   "source": [
    "eng_vocab= set()\n",
    "for sent in df.English:\n",
    "    for word in sent.split():\n",
    "        if word not in eng_vocab:\n",
    "            eng_vocab.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "475ba93e-2bd7-4b60-a23d-e72529aed56c"
   },
   "outputs": [],
   "source": [
    "mar_vocab= set()\n",
    "for sent in df.Marathi:\n",
    "    for word in sent.split():\n",
    "        if word not in mar_vocab:\n",
    "            mar_vocab.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93e0d31a-4974-4c3d-92b0-78655700ecfe"
   },
   "outputs": [],
   "source": [
    "len(eng_vocab), len(mar_vocab), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2e7b595-9a5a-41b8-86bd-08918e5dcc56"
   },
   "outputs": [],
   "source": [
    "# for zero padding add 1 in them\n",
    "ENG_VOCAB_SIZE= len(eng_vocab)+1\n",
    "MAR_VOCAB_SIZE= len(mar_vocab)+1\n",
    "print(ENG_VOCAB_SIZE)\n",
    "print(MAR_VOCAB_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17e310ae-b882-49fd-b1ad-22d57d32b8e4"
   },
   "source": [
    "### Create dictionary for words and their indexes then we can convert text into numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0c840cd-2c39-4209-815a-eda5185d8392"
   },
   "source": [
    "#### First we need sorted words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "303bef8d-dfd4-4e0d-a9ea-f2cc513b5fc2"
   },
   "outputs": [],
   "source": [
    "eng_words = sorted(list(eng_vocab))\n",
    "mar_words = sorted(list(mar_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f36966f2-c5ac-4879-be50-be3d4b00b61c"
   },
   "source": [
    "#### Word to number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28320197-95ff-4a08-83eb-66454e0916a5"
   },
   "outputs": [],
   "source": [
    "# create english and marathi dicts\n",
    "eng_word_index = dict((w, i) for i, w in enumerate(eng_words))\n",
    "mar_word_index = dict((w, i) for i, w in enumerate(mar_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJ20wjUwljqn"
   },
   "outputs": [],
   "source": [
    "mar_word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5e5870f-6b8b-4dc7-828b-99165eb564f7"
   },
   "source": [
    "#### Number to word\n",
    "*  we will need this one at time of creating text from predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19a875ee-0995-4b8c-bfc5-934016c4e6cc"
   },
   "outputs": [],
   "source": [
    "eng_index_word = dict((i, w) for i, w in enumerate(eng_words))\n",
    "mar_index_word = dict((i,w) for i, w in enumerate(mar_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yq1W1V3ZmC4X"
   },
   "outputs": [],
   "source": [
    "mar_index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aae52575-dc0e-49cc-b092-02fa4b38f8f3"
   },
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4f1f048e-478f-4cb9-9709-a063f5a9f832"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(df.English, df.Marathi, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33bea87a-2739-43fb-97de-7eea2fd2bc95"
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65197549-2162-4447-ac85-1768f5672f40"
   },
   "source": [
    "## Create data generator \n",
    "* if we create array of 3d shape with our vocab size this will give us out of memmory error \n",
    "* And it is always best to use batches to train it will make process faster \n",
    "* Insted of passing all data in model which may run out of memory we create data generator which will create data batches at time of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce2e0fd8-9cb2-47c3-b7b8-7b4f94c9332f"
   },
   "source": [
    "#### prepare input for encoder decoder [refer](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)\n",
    "* Turn the sentences into 3 Numpy arrays, encoder_input_data, decoder_input_data, decoder_target_data:\n",
    ">* encoder_input_data is a 3D array of shape (num_pairs,max_english_sentence_length, num_english_characters) containing a one-hot vectorization of the English sentences.\n",
    ">* decoder_input_data is a 3D array of shape (num_pairs, max_french_sentence_length, num_french_characters) containg a one-hot vectorization of the French sentences.\n",
    ">* decoder_target_data is the same as decoder_input_data but offset by one timestep. decoder_target_data[:, t, :] will be the same as decoder_input_data[:, t + 1, :].\n",
    "* 2) Train a basic LSTM-based Seq2Seq model to predict decoder_target_data given encoder_input_data and decoder_input_data. Our model uses teacher forcing.\n",
    "* 3) Decode some sentences to check that the model is working (i.e. turn samples from encoder_input_data into corresponding samples from decoder_target_data).\n",
    "\n",
    "* We use a technique called “Teacher Forcing” wherein the input at each time step is given as the actual output (and not the predicted output) from the previous time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXmGslnnwF8u"
   },
   "source": [
    "#### Before we go ahead lets define some things we need for data generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmQkNBlywVnU"
   },
   "outputs": [],
   "source": [
    "### Get lengths of each sentence in list\n",
    "eng_len_list=df.English.apply(lambda x: len(x.split())).to_list()\n",
    "\n",
    "mar_len_list=df.Marathi.apply(lambda x: len(x.split())).to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jjZ9wIpw4z3"
   },
   "outputs": [],
   "source": [
    "# get max length \n",
    "np.max(mar_len_list), np.max(eng_len_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25658248-98a8-48de-88e5-8d0ad1c1ccf9"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE= 64\n",
    "max_eng_len =  np.max(eng_len_list)\n",
    "max_mar_len =  np.max(mar_len_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qM1VRPLkxgkg"
   },
   "outputs": [],
   "source": [
    "max_eng_len, max_mar_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVQdFK15kTsu"
   },
   "source": [
    "## Get data generator fuction from [keras](https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly) team\n",
    "* also visit [here](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html) to see how keras created this input's for encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "578042e4-1fb3-490f-815d-1560be84bb39"
   },
   "outputs": [],
   "source": [
    "def data_batch_generator(x, y, batch_size=BATCH_SIZE):\n",
    "    while True:\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size,max_eng_len ), dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_mar_len), dtype='float32')\n",
    "            # one hot encoded target data beause dense layer with softmax will give only one output at a time\n",
    "            decoder_target_data = np.zeros((batch_size, max_mar_len, MAR_VOCAB_SIZE), dtype='float32' )\n",
    "            for j, (eng_text, mar_text) in enumerate(zip(x[i:i+batch_size], y[i:i+batch_size])):\n",
    "                for t , word in enumerate(eng_text.split()):\n",
    "                    encoder_input_data[j,t] = eng_word_index[word]\n",
    "                for t, word in enumerate(mar_text.split()):\n",
    "                    if t < len(mar_text.split()) - 1:\n",
    "                        decoder_input_data[j,t]= mar_word_index[word]\n",
    "                    if t>0:\n",
    "                        # This is decoder target output which is one step ahead of decoder input \n",
    "                        # it does not have EOS token \n",
    "                        decoder_target_data[j,t-1, mar_word_index[word]] = 1.\n",
    "            yield ([encoder_input_data, decoder_input_data], decoder_target_data)      \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "984a1ae0-6540-4359-bd9d-de6af0fd13a6"
   },
   "source": [
    "# LSTM Encoder Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "549a9719-6cfa-45e8-8b0b-6a8d54f8a4dd"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Embedding\n",
    "from tensorflow.keras import Input, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "571de098-9471-46c3-85df-9e8a330c0b14"
   },
   "outputs": [],
   "source": [
    "# Eoncoder\n",
    "encoder_input = Input(shape=(None, ))\n",
    "encoder_embd = Embedding(ENG_VOCAB_SIZE,100, mask_zero=True)(encoder_input)\n",
    "encoder_lstm = LSTM(100, return_state=True)\n",
    "encoder_output,state_h, state_c = encoder_lstm(encoder_embd)\n",
    "\n",
    "## Now take only states and create context vector\n",
    "encoder_states= [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_input = Input(shape=(None,))\n",
    "# For zero padding we have added +1 in marathi vocab size\n",
    "decoder_embd = Embedding(MAR_VOCAB_SIZE, 100, mask_zero=True)\n",
    "decoder_embedding= decoder_embd(decoder_input)\n",
    "decoder_lstm = LSTM(100, return_state=True,return_sequences=True )\n",
    "# just take output of this decoder dont need self states\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "# here this is going to predicct so we can add dense layer here\n",
    "# here we want to convert predicted numbers into probability so use softmax\n",
    "decoder_dense= Dense(MAR_VOCAB_SIZE, activation='softmax')\n",
    "# We will again feed predicted output into decoder to predict its next word\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model1 = Model([encoder_input, decoder_input], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "id": "a7a63875-09b7-42a7-ac9a-388709d1f030",
    "outputId": "e7360f26-b774-4607-9a14-ceae9f02123c"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import  plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "plot_model(model1,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bb955cbf-fd5c-4c9a-8ade-3ba801a9fe42"
   },
   "outputs": [],
   "source": [
    "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0VaoiMfNARW"
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"/content/drive/MyDrive/rnn/machine_translation/Encoder_Decoder/model_checkpoints/\", monitor='val_accuracy')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "\n",
    "callbacks_list = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZA--PicSrFuY"
   },
   "source": [
    "#### **IMP NOTE** - here to avoid unknown samples need to use sptes per epoch else model will fit for infinite samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20a44f08-c97f-499a-9a19-10e1a3bdd87d"
   },
   "outputs": [],
   "source": [
    "steps_per_epoch= np.ceil(len(X_train)/BATCH_SIZE)\n",
    "steps_per_epoch_val = np.ceil(len(X_train)/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T_LoE9qlrmxW",
    "outputId": "8f9a6850-3630-4775-9d45-16e7001d83c7"
   },
   "outputs": [],
   "source": [
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NyudmuhTr6iA",
    "outputId": "eca4dffc-81e5-4b83-8ac2-f5f54e0417c8"
   },
   "outputs": [],
   "source": [
    "EPOCHS= 30 #@param {type:'slider',min:10,max:100, step:10 }\n",
    "EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29a3ac8d-6db9-4620-acea-4ffdb2c4d3ce",
    "outputId": "905fe615-72ea-4a7e-cf9e-b53c7aab6edb"
   },
   "outputs": [],
   "source": [
    "history1= model1.fit(data_batch_generator(X_train,y_train), \n",
    "                       epochs=EPOCHS,\n",
    "                       steps_per_epoch= steps_per_epoch,\n",
    "                     validation_data=data_batch_generator(X_test, y_test, BATCH_SIZE),\n",
    "                       validation_steps=steps_per_epoch_val,\n",
    "                     callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjZXwzehx7d2"
   },
   "outputs": [],
   "source": [
    "model1.save_weights('model1_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qYpoNVAbuEf"
   },
   "outputs": [],
   "source": [
    "model1.load_weights('model1_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBiNqCFhT1LJ"
   },
   "source": [
    "# Model2\n",
    "Now we will try to improve its accurcy with changing some units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6mEo4AJT-AX"
   },
   "outputs": [],
   "source": [
    "# Eoncoder\n",
    "encoder_input = Input(shape=(None, ))\n",
    "encoder_embd = Embedding(ENG_VOCAB_SIZE,1000, mask_zero=True)(encoder_input)\n",
    "encoder_lstm = LSTM(250, return_state=True)\n",
    "encoder_output,state_h, state_c = encoder_lstm(encoder_embd)\n",
    "\n",
    "## Now take only states and create context vector\n",
    "encoder_states= [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_input = Input(shape=(None,))\n",
    "# For zero padding we have added +1 in marathi vocab size\n",
    "decoder_embd = Embedding(MAR_VOCAB_SIZE, 1000, mask_zero=True)\n",
    "decoder_embedding= decoder_embd(decoder_input)\n",
    "decoder_lstm = LSTM(250, return_state=True,return_sequences=True )\n",
    "# just take output of this decoder dont need self states\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "# here this is going to predicct so we can add dense layer here\n",
    "# here we want to convert predicted numbers into probability so use softmax\n",
    "decoder_dense= Dense(MAR_VOCAB_SIZE, activation='softmax')\n",
    "# We will again feed predicted output into decoder to predict its next word\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model2 = Model([encoder_input, decoder_input], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "id": "dnObzoz9T-AZ",
    "outputId": "b4adb6ef-11e7-408a-e374-5bd63bd630b1"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import  plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "plot_model(model2,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PA_dopDpT-Ab"
   },
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKT-XO8yT-Ac"
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"/content/drive/MyDrive/rnn/machine_translation/Encoder_Decoder/model_checkpoints/model2/\", monitor='val_accuracy')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "\n",
    "callbacks_list = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNFCSoWiT-Ac"
   },
   "outputs": [],
   "source": [
    "steps_per_epoch= np.ceil(len(X_train)/BATCH_SIZE)\n",
    "steps_per_epoch_val = np.ceil(len(X_train)/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Stxp05XFT-Ad",
    "outputId": "aab8319c-0f8c-4654-e7d8-e70476ced1f8"
   },
   "outputs": [],
   "source": [
    "EPOCHS= 30 #@param {type:'slider',min:10,max:100, step:10 }\n",
    "EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k1WWj5LeT-Ae",
    "outputId": "c1f239f9-ebe2-4fca-9caf-ec53e0ec7a08"
   },
   "outputs": [],
   "source": [
    "history2= model2.fit(data_batch_generator(X_train,y_train), \n",
    "                       epochs=EPOCHS,\n",
    "                       steps_per_epoch= steps_per_epoch,\n",
    "                     validation_data=data_batch_generator(X_test, y_test, BATCH_SIZE),\n",
    "                       validation_steps=steps_per_epoch_val,\n",
    "                     callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q46qGS07T31R"
   },
   "outputs": [],
   "source": [
    "model2.save_weights('/content/drive/MyDrive/rnn/machine_translation/Encoder_Decoder/saved_models/2_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ird9oNUzT3y1"
   },
   "outputs": [],
   "source": [
    "model2.load_weights('2_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZ4YOcAg-vGY"
   },
   "source": [
    "## Inference model\n",
    "*  As we trained our enoder decoder do same for prdiction means apply encoder on input sent and applying decoder on target sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a06gEHmPanYD"
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_input, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7uAfAnoa4KS"
   },
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(100,))\n",
    "decoder_state_input_c= Input(shape=(100,))\n",
    "decoder_states_input= [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_embd2 = decoder_embd(decoder_input)\n",
    "\n",
    "decoder_output2,state_h2, state_c2 = decoder_lstm(dec_embd2, initial_state=decoder_states_input)\n",
    "deccoder_states2= [state_h2, state_c2]\n",
    "\n",
    "decoder_output2 = decoder_dense(decoder_output2)\n",
    "\n",
    "decoder_model = Model(\n",
    "                      [decoder_input]+decoder_states_input,\n",
    "                      [decoder_output2]+ deccoder_states2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIL9b9p1JCaN"
   },
   "source": [
    "# To predict we have to encoder text first then pass than to decoder we can get predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aq7hdBLWcQY8"
   },
   "outputs": [],
   "source": [
    "def get_predicted_sentence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = mar_word_index['sos']\n",
    "    \n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        # convert max index number to marathi word\n",
    "        sampled_char = mar_index_word[sampled_token_index]\n",
    "        # aapend it ti decoded sent\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "        \n",
    "        # Exit condition: either hit max length or find stop token.\n",
    "        if (sampled_char == 'eos' or len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "        \n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        \n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnZyZa33eLOH"
   },
   "outputs": [],
   "source": [
    "test_gen= data_batch_generator(X_test,y_test,batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4hQIREkIQnF"
   },
   "outputs": [],
   "source": [
    "Actual_test_sent = X_test.to_list()\n",
    "Actual_test_trans= y_test.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6zpJJe9BbkE"
   },
   "outputs": [],
   "source": [
    "test_inputs=[]\n",
    "test_outputs=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4eORpxGSR2k"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for (input, output),_ in tqdm(iter(test_gen)):\n",
    "    test_inputs.append(input)\n",
    "    test_outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input_file.txt\", 'w') as f:\n",
    "    for s in test_inputs:\n",
    "        f.write(str(s) + '\\n')\n",
    "with open(\"output_file.txt\", 'w') as f:\n",
    "    for s in test_outputs:\n",
    "        f.write(str(s) + '\\n')\n",
    "\n",
    "with open(\"input_file.txt\", 'r') as f:\n",
    "    test_inputs = [line.rstrip('\\n') for line in f]\n",
    "with open(\"output_file.txt\", 'r') as f:\n",
    "    test_outputs = [line.rstrip('\\n') for line in f] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nuVMzFTZDvrU",
    "outputId": "adfbc87b-78ef-4097-a5fe-67d03999003d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"English Senteces:\", Actual_test_sent[0])\n",
    "print(\"Actual Marathi Sentence:\", Actual_test_trans[0][:-4])\n",
    "print(\"Predicted Marathi Translation:\", get_predicted_sentence(test_inputs[0])[:-4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L8-i8QpRmVBF",
    "outputId": "ce8185df-6299-4dfa-998d-ca4d3d2127f8"
   },
   "outputs": [],
   "source": [
    "print(\"English Senteces:\", Actual_test_sent[1])\n",
    "print(\"Actual Marathi Sentence:\", Actual_test_trans[1][:-4])\n",
    "print(\"Predicted Marathi Translation:\", get_predicted_sentence(test_inputs[1])[:-4])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ce2e0fd8-9cb2-47c3-b7b8-7b4f94c9332f"
   ],
   "name": "Training_Encoder_Decoder.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
